{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fb9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary packages\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import re\n",
    "import os\n",
    "from spacy import displacy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9c5656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0\n",
      "0                     a pizza with cheese and spinach\n",
      "1                          a pie with a slice missing\n",
      "2                     a sandwich with meat and cheese\n",
      "3                       a cheeseburger on a newspaper\n",
      "4                             a white cake on a plate\n",
      "5        a hand holding a paper plate of french fries\n",
      "6              a large burger with lettuce and tomato\n",
      "7                             a hand holding a cookie\n",
      "8           a slice of cake with whipped cream on top\n",
      "9               a stack of pancakes with fruit on top\n",
      "10                  a plate of seafood and vegetables\n",
      "11              a person holding a bunch of cucumbers\n",
      "12                       slices of oranges on a plate\n",
      "13  a group of cupcakes with frosting and candy on...\n",
      "14                                a group of tomatoes\n",
      "15                            a close-up of some eggs\n",
      "16             a chocolate donut with sprinkles on it\n",
      "17                              a bowl of noodle soup\n",
      "18                           a fish with an egg on it\n",
      "19           a cheeseburger with tomatoes and lettuce\n",
      "20                  a plate of seafood and vegetables\n",
      "21              a person holding a bunch of cucumbers\n",
      "22                       slices of oranges on a plate\n",
      "23  a group of cupcakes with frosting and candy on...\n",
      "24                                a group of tomatoes\n",
      "25                            a close-up of some eggs\n",
      "26             a chocolate donut with sprinkles on it\n",
      "27                              a bowl of noodle soup\n",
      "28                           a fish with an egg on it\n",
      "29           a cheeseburger with tomatoes and lettuce\n",
      "30                   a bowl of noodles and vegetables\n",
      "31                  a person holding a slice of pizza\n",
      "32                                    a tray of sushi\n",
      "33                        a bowl of soup with a spoon\n",
      "34                a table with food and a can of soda\n"
     ]
    }
   ],
   "source": [
    "FOODKEEPER_PATH = \"datasets/FoodKeeper-Data.xls\"\n",
    "TRAINING_DATA_PATH = \"datasets/response.txt\"\n",
    "MODEL_PATH = \"output/model-last\"\n",
    "TEST_DATA_PATH = \"datasets/test_data.csv\"\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#creates a blank pipeline of a given language class - english\n",
    "#function is twin of spacy.load()\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Read a comma-separated values (csv) file into DataFrame\n",
    "#Also supports optionally iterating or breaking the file into chunks\n",
    "food_data = pd.read_excel(FOODKEEPER_PATH, sheet_name = \"Product\")\n",
    "training_data = pd.read_csv(TRAINING_DATA_PATH,index_col = False, header = None)\n",
    "test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "print(training_data)\n",
    "\n",
    "#loop through and count the specific entities\n",
    "keywords = [] #'chicken', 'milk', 'butter', 'cheese'\n",
    "sampleData = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941678fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d31680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['butter',\n",
       " 'buttermilk',\n",
       " 'cheese',\n",
       " 'coffee creamer',\n",
       " 'cottage cheese',\n",
       " 'cream cheese',\n",
       " 'cream',\n",
       " 'dips',\n",
       " 'egg substitutes',\n",
       " 'eggnog',\n",
       " 'eggs',\n",
       " 'egg dishes',\n",
       " 'kefir',\n",
       " 'margarine',\n",
       " 'milk',\n",
       " 'pudding',\n",
       " 'sour cream',\n",
       " 'whipped cream',\n",
       " 'whipped topping',\n",
       " 'yogurt',\n",
       " 'beef',\n",
       " 'lamb',\n",
       " 'veal',\n",
       " 'pork',\n",
       " 'goat',\n",
       " 'venison',\n",
       " 'variety meats',\n",
       " 'bacon',\n",
       " 'corned beef',\n",
       " 'ham',\n",
       " 'hot dogs',\n",
       " 'sausage',\n",
       " 'stuffed  raw pork chops',\n",
       " 'raw kabobs with vegetables',\n",
       " 'jerky',\n",
       " 'meat products',\n",
       " 'retort pouches boxes',\n",
       " 'chicken',\n",
       " 'turkey',\n",
       " 'ground turkey chicken',\n",
       " 'chicken parts',\n",
       " 'turkey parts',\n",
       " 'duckling',\n",
       " 'goose',\n",
       " 'pheasant',\n",
       " 'quail',\n",
       " 'capon',\n",
       " 'cornish hens',\n",
       " 'giblets',\n",
       " 'stuffed  raw chicken breasts',\n",
       " 'turducken',\n",
       " 'chicken nuggets  patties',\n",
       " 'cooked poultry dishes',\n",
       " 'fried chicken',\n",
       " 'poultry pieces',\n",
       " 'rotisserie chicken',\n",
       " 'canned chicken',\n",
       " 'lean fish',\n",
       " 'fatty fish',\n",
       " 'caviar',\n",
       " 'cooked fish',\n",
       " 'surimi seafood',\n",
       " 'scallops',\n",
       " 'shrimp  crayfish',\n",
       " 'squid',\n",
       " 'shucked clams  mussels  and oysters',\n",
       " 'crab meat',\n",
       " 'crab legs',\n",
       " 'live clams  mussels  crab  and oysters',\n",
       " 'fresh whole lobster',\n",
       " 'fresh lobster tails',\n",
       " 'fresh clams  mussels  oysters',\n",
       " 'cooked shellfish',\n",
       " 'herring',\n",
       " 'fish',\n",
       " 'tofu',\n",
       " 'miso',\n",
       " 'soy flour',\n",
       " 'textured soy protein',\n",
       " 're-hydrated textured soy protein',\n",
       " 'leftovers',\n",
       " 'commercial brand vacuum-packed dinners',\n",
       " 'cooked pasta',\n",
       " 'cooked rice',\n",
       " 'fruit  cut',\n",
       " 'guacamole',\n",
       " 'hummus',\n",
       " 'luncheon meat poultry',\n",
       " 'main dishes meals',\n",
       " 'meats',\n",
       " 'olives',\n",
       " 'pate',\n",
       " 'soup  stews',\n",
       " 'casseroles',\n",
       " 'commercial bread products',\n",
       " 'tortillas',\n",
       " 'commercial cakes and muffins',\n",
       " 'cheesecake',\n",
       " 'cookies',\n",
       " 'dairy filled eclairs',\n",
       " 'doughnuts',\n",
       " 'fruit cake',\n",
       " 'pastries  danish',\n",
       " 'cream pies',\n",
       " 'chiffon pies',\n",
       " 'fruit pies',\n",
       " 'pies',\n",
       " 'quiche',\n",
       " 'baking powder',\n",
       " 'baking soda',\n",
       " 'biscuit pancake mix',\n",
       " 'cake  brownie  bread mixes',\n",
       " 'chocolate',\n",
       " 'cocoa and cocoa mixes',\n",
       " 'cornmeal',\n",
       " 'cornstarch',\n",
       " 'flour',\n",
       " 'frosting icing',\n",
       " 'gelatin',\n",
       " 'oils',\n",
       " 'nut oils',\n",
       " 'vegetable oil sprays',\n",
       " 'shortening',\n",
       " 'tamarind paste',\n",
       " 'chili powder',\n",
       " 'seasoning blends',\n",
       " 'flavored herb mixes',\n",
       " 'garlic',\n",
       " 'herbs',\n",
       " 'spice spices',\n",
       " 'sugar',\n",
       " 'sugar substitutes',\n",
       " 'tapiocas',\n",
       " 'tube cans',\n",
       " 'ready-to-bake pie crust',\n",
       " 'cookie dough',\n",
       " 'apples',\n",
       " 'apricots',\n",
       " 'avocados',\n",
       " 'bananas',\n",
       " 'berries',\n",
       " 'blueberries',\n",
       " 'cherimoya',\n",
       " 'citrus fruit',\n",
       " 'coconut',\n",
       " 'coconuts',\n",
       " 'cranberries',\n",
       " 'dates',\n",
       " 'grapes',\n",
       " 'guava',\n",
       " 'kiwi fruit',\n",
       " 'melons',\n",
       " 'papaya  mango  feijoa  passionfruit  casaha melon',\n",
       " 'peaches  nectarines  plums  pears  sapote',\n",
       " 'pineapple',\n",
       " 'plantains',\n",
       " 'pomegranate',\n",
       " 'artichokes  whole',\n",
       " 'asparagus',\n",
       " 'bamboo shoots',\n",
       " 'beans and peas',\n",
       " 'beets',\n",
       " 'bok choy',\n",
       " 'broccoli and broccoli raab (rapini)',\n",
       " 'brussels sprouts',\n",
       " 'cabbage',\n",
       " 'carrots  parsnips',\n",
       " 'cauliflower',\n",
       " 'celery',\n",
       " 'corn on the cob',\n",
       " 'cucumbers',\n",
       " 'eggplant',\n",
       " 'ginger root',\n",
       " 'greens',\n",
       " 'leeks',\n",
       " 'lettuce',\n",
       " 'mushrooms',\n",
       " 'okra',\n",
       " 'onions',\n",
       " 'peppers',\n",
       " 'potatoes',\n",
       " 'pumpkins',\n",
       " 'radishes',\n",
       " 'rhubarb',\n",
       " 'rutabagas',\n",
       " 'squash',\n",
       " 'tamarind',\n",
       " 'taro',\n",
       " 'tomatoes',\n",
       " 'turnips',\n",
       " 'yuca cassava',\n",
       " 'dough',\n",
       " 'frozen potato products',\n",
       " 'frozen pretzels',\n",
       " 'fruits',\n",
       " 'ice cream',\n",
       " 'ice pops',\n",
       " 'juice concentrates',\n",
       " 'lobster tails',\n",
       " 'pancakes  waffles',\n",
       " 'sausages',\n",
       " 'sherbet  sorbet',\n",
       " 'shrimp  shellfish',\n",
       " 'soy crumbles and hot dogs',\n",
       " 'soy meat substitutes',\n",
       " 'tempeh',\n",
       " 'frozen entrees',\n",
       " 'vegetables',\n",
       " 'fresh pasta',\n",
       " 'beans',\n",
       " 'lentils',\n",
       " 'pasta',\n",
       " 'dry egg noodles',\n",
       " 'peas',\n",
       " 'rice',\n",
       " 'barbecue sauce',\n",
       " 'chutney',\n",
       " 'cream sauces  milk solids',\n",
       " 'dry gravy mixes',\n",
       " 'gravy',\n",
       " 'honey',\n",
       " 'horseradish',\n",
       " 'jams  jellies  and preserves',\n",
       " 'ketchup  cocktail  chili sauce',\n",
       " 'marinades',\n",
       " 'mayonnaise',\n",
       " 'mustard',\n",
       " 'pickles',\n",
       " 'pesto',\n",
       " 'salad dressings',\n",
       " 'salsa',\n",
       " 'sauce mixes',\n",
       " 'spaghetti sauce',\n",
       " 'soy sauce teriyaki sauce',\n",
       " 'vinegar',\n",
       " 'worcestershire sauce',\n",
       " 'jars pouches',\n",
       " 'fruit',\n",
       " 'dinners',\n",
       " 'cereal  dry mixes',\n",
       " 'formula',\n",
       " 'liquid concentrate ready-to-feed formula',\n",
       " 'applesauce',\n",
       " 'bacon bits',\n",
       " 'canned goods',\n",
       " 'cereal',\n",
       " 'chocolate syrup',\n",
       " 'crackers',\n",
       " 'graham cracker animal cracker',\n",
       " 'fruits  dried',\n",
       " 'gummy (fruit) snacks',\n",
       " 'marshmallows',\n",
       " 'marshmallow crème',\n",
       " 'molasses',\n",
       " 'nuts',\n",
       " 'peanut butter',\n",
       " 'pectin',\n",
       " 'popcorn',\n",
       " 'potato chips',\n",
       " 'pretzels',\n",
       " 'pudding mixes',\n",
       " 'soup mixes',\n",
       " 'sun dried tomatoes',\n",
       " 'syrup',\n",
       " 'toaster pastries',\n",
       " 'coffee',\n",
       " 'diet powder mixes and drink mixes',\n",
       " 'fruit juice in cartons  fruit drinks  punch',\n",
       " 'juice  boxes',\n",
       " 'nectar',\n",
       " 'soda',\n",
       " 'soy rice beverage',\n",
       " 'tea',\n",
       " 'water',\n",
       " 'kumquats',\n",
       " 'bagged greens',\n",
       " 'tahini',\n",
       " 'egg salad',\n",
       " 'potato salad',\n",
       " 'seafood salads',\n",
       " 'chicken salad',\n",
       " 'ham salad',\n",
       " 'pasta salad',\n",
       " 'soy milk',\n",
       " 'yams sweet potatoes',\n",
       " 'kale',\n",
       " 'quinoa',\n",
       " 'almond milk',\n",
       " 'rice milk',\n",
       " 'coconut milk',\n",
       " 'turkey bacon',\n",
       " 'fruit cocktail',\n",
       " 'black bean sauce',\n",
       " 'oyster sauce',\n",
       " 'hoisin sauce',\n",
       " 'pork roll',\n",
       " 'almonds',\n",
       " 'cashews',\n",
       " 'macadamias',\n",
       " 'peanuts',\n",
       " 'pecans',\n",
       " 'pistachios',\n",
       " 'walnuts',\n",
       " 'lime juice',\n",
       " 'lemon juice',\n",
       " 'bagel',\n",
       " 'muffin',\n",
       " 'coconut oil',\n",
       " 'orange juice',\n",
       " 'roasted red peppers',\n",
       " 'whole wheat flour',\n",
       " 'whole wheat bread',\n",
       " 'red wine',\n",
       " 'white wine',\n",
       " 'dry stuffing mix',\n",
       " 'powdered milk',\n",
       " 'almond butter',\n",
       " 'cashew butter',\n",
       " 'salt',\n",
       " 'black pepper',\n",
       " 'cajun seasoning blend',\n",
       " 'cinnamon',\n",
       " 'cumin',\n",
       " 'garlic powder',\n",
       " 'onion powder',\n",
       " 'nutmeg',\n",
       " 'nacho cheese',\n",
       " 'star fruit',\n",
       " 'prickly pear',\n",
       " 'pitaya dragon fruit',\n",
       " 'strawberries',\n",
       " 'raspberries',\n",
       " 'cherries',\n",
       " 'broth',\n",
       " 'beef broth stock consommé',\n",
       " 'chicken broth stock consommé',\n",
       " 'vegetable stock broth',\n",
       " 'ricotta',\n",
       " 'baby carrots',\n",
       " 'jicama',\n",
       " 'kimchi',\n",
       " 'kohlrabi',\n",
       " 'watermelon',\n",
       " 'cantaloupe',\n",
       " 'honeydew',\n",
       " 'refried beans',\n",
       " 'relish',\n",
       " 'tomato paste',\n",
       " 'tapenade',\n",
       " 'flaxseed',\n",
       " 'lemongrass',\n",
       " 'cilantro',\n",
       " 'mint',\n",
       " 'basil',\n",
       " 'oregano',\n",
       " 'rosemary',\n",
       " 'chives',\n",
       " 'thyme',\n",
       " 'salami',\n",
       " 'canadian bacon',\n",
       " 'canola oil',\n",
       " 'almond oil',\n",
       " 'sunflower oil',\n",
       " 'grapeseed oil',\n",
       " 'duck fat',\n",
       " 'bacon grease',\n",
       " 'frying oil',\n",
       " 'almond extract',\n",
       " 'cinnamon extract',\n",
       " 'lemon extract',\n",
       " 'pure vanilla extract',\n",
       " 'butter flavor',\n",
       " 'coconut flavor',\n",
       " '\"genuine\" maple syrup',\n",
       " 'apple juice',\n",
       " 'carrot juice',\n",
       " 'hard liquors',\n",
       " 'cream liquors',\n",
       " 'macaroons',\n",
       " 'string cheese',\n",
       " 'vegan cheddar cheese',\n",
       " 'quark',\n",
       " 'zucchini',\n",
       " 'hot peppers',\n",
       " 'bean sprouts',\n",
       " 'swiss chard',\n",
       " 'puff pastry',\n",
       " 'biscuits',\n",
       " 'pie crust',\n",
       " 'granola',\n",
       " 'pork rinds',\n",
       " 'apple cider vinegar',\n",
       " 'nutrition supplement drinks',\n",
       " 'hot sauce',\n",
       " 'thai red curry paste',\n",
       " 'yeast',\n",
       " 'cranberry sauce',\n",
       " 'vegetable juice',\n",
       " 'marinated vegetables',\n",
       " 'pizza',\n",
       " 'chia seeds',\n",
       " 'bread',\n",
       " 'amaranth',\n",
       " 'barley',\n",
       " 'buckwheat',\n",
       " 'farro',\n",
       " 'millet',\n",
       " 'oats',\n",
       " 'rye',\n",
       " 'sorghum',\n",
       " 'spelt',\n",
       " 'teff',\n",
       " 'rabbit',\n",
       " 'spaghetti squash',\n",
       " 'garam masala',\n",
       " 'tomato sauce',\n",
       " 'cherry tomatoes',\n",
       " 'coconut cream',\n",
       " 'coleslaw',\n",
       " 'pumpkin seeds',\n",
       " 'sunflower seeds',\n",
       " 'parsley',\n",
       " 'sesame oil',\n",
       " 'sesame seeds',\n",
       " 'capers',\n",
       " 'chocolate hazlenut spread',\n",
       " 'roasted nuts (peanuts  cashews  almonds)',\n",
       " 'tuna',\n",
       " 'seafood',\n",
       " 'pimento cheese',\n",
       " 'bison',\n",
       " 'salad dressing',\n",
       " 'grits',\n",
       " 'cheese curds',\n",
       " 'vegetable soup',\n",
       " 'chorizo',\n",
       " 'cinnamon rolls',\n",
       " 'cooking wine',\n",
       " 'bulgur',\n",
       " 'bratwurst',\n",
       " 'edamame',\n",
       " 'breadcrumbs',\n",
       " 'ghee',\n",
       " 'corn syrup',\n",
       " 'instant breakfast drinks',\n",
       " 'pine nuts',\n",
       " 'coconut flour',\n",
       " 'polenta',\n",
       " 'cereal granola bars',\n",
       " 'coconut water',\n",
       " 'celery root',\n",
       " 'apple cider',\n",
       " 'yuzu juice',\n",
       " 'yuzu',\n",
       " 'pastrami',\n",
       " 'kugel',\n",
       " 'avocado oil',\n",
       " 'balsamic vinegar',\n",
       " 'aioli',\n",
       " 'base',\n",
       " 'anchovies',\n",
       " 'radicchio',\n",
       " 'prosciutto',\n",
       " 'arugula',\n",
       " 'mung bean',\n",
       " 'croutons']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# makes a list of all the keywords from FoodKeeper database\n",
    "def foodKeeperInfo():              \n",
    "    keywords = []\n",
    "    for word in food_data['Name']:\n",
    "        word = word.replace(\" or \", \" \")\n",
    "        word = re.sub('[/,]', ' ', word)\n",
    "        word = word.lstrip()\n",
    "        word = word.rstrip()\n",
    "\n",
    "        if word.lower() not in keywords: \n",
    "            keywords.append(word.lower())\n",
    "\n",
    "    #print(\"Total foodkeeper food names: \" + str(len(keywords)))        \n",
    "    #for element in sorted(keywords):\n",
    "        #print(element)\n",
    "        \n",
    "    return keywords\n",
    "\n",
    "foodKeeperKeywordsTest = foodKeeperInfo()\n",
    "foodKeeperKeywordsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40200bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gets the functions from foodKeeperFunctions file\n",
    "%run foodKeeperFunctionsInsta.ipynb\n",
    "\n",
    "def findNewKeywords(post, keywords):\n",
    "    foodkeeperKeys = foodKeeperInfo()\n",
    "    x = post.split()\n",
    "    word = \"\"\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "    #for i in range(len(x)):\n",
    "        z = 1\n",
    "        if x[i] in foodkeeperKeys:\n",
    "            word = x[i]\n",
    "        try:\n",
    "            foundBiWord = x[i] + \" \" + x[i+1]\n",
    "            if foundBiWord in foodkeeperKeys: #keywords\n",
    "                word = foundBiWord\n",
    "                z = 2\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            foundTriWord = x[i] + \" \" + x[i+1] + \" \" + x[i+2]\n",
    "            if foundTriWord in foodkeeperKeys: #keywords:\n",
    "                word = foundTriWord\n",
    "                z = 3\n",
    "        except:\n",
    "            pass\n",
    "        i += z\n",
    "        \n",
    "        if word not in keywords and word != \"\":\n",
    "            keywords.append(word)\n",
    "    return keywords\n",
    "\n",
    "def ent_recognize(text):\n",
    "    doc = nlp(text)\n",
    "    displacy.render(doc,style = \"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c99eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21940/3760646366.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmyInstaPost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mfindNewKeywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#print(test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "keywordRanker = {}\n",
    "myInstaPost = []\n",
    "keywords = ['noodles', 'shrimp', 'soup']\n",
    "x = convertToTrainingFormat(training_data[0][20], keywords)\n",
    "myInstaPost.append(x)\n",
    "for elem in myInstaPost:\n",
    "    print(elem)\n",
    "    findNewKeywords(elem[0], test)\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a91640",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getCommonVerbs(data):\n",
    "    import en_core_web_sm\n",
    "    nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "    count = 0\n",
    "    myVerbs = {}\n",
    "    for i in range(len(data[0])):\n",
    "        doc = nlp2(data[0][i])\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                if token.text in myVerbs:\n",
    "                    myVerbs[token.text] = myVerbs[token.text] + 1\n",
    "                else:\n",
    "                    if token.text not in nlp2.Defaults.stop_words:\n",
    "                        myVerbs[token.text] = 1\n",
    "        \n",
    "\n",
    "    topVerbs = dict(sorted(myVerbs.items(), key = lambda item: item[1], reverse=True)[:10])\n",
    "    return [key for key in topVerbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39373911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(tweet):\n",
    "    #Converts a tweet to lowercase, replaces anyusername w/ <USERNAME> and URLS with <URL>\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('@[a-zA-z0-9]*', '', tweet)              # <USERNAME>\n",
    "    tweet = re.sub('http[a-zA-z0-9./:]*', '', tweet)       # <URL>\n",
    "    tweet = re.sub('[.,-]*', '', tweet)\n",
    "    tweet = re.sub('&amp;', 'and', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "noEntity= []\n",
    "\n",
    "keywordRanker = {}\n",
    "\n",
    "def trainModel(data):\n",
    "    #Initialize all the variables\n",
    "    keywords = [] #foodKeeperInfo()\n",
    "    oldKeywords = []\n",
    "    newKeywords = []\n",
    "    \n",
    "      \n",
    "    \n",
    "    commonVerbs = getCommonVerbs(data) \n",
    "    print(commonVerbs)\n",
    "    print(\"Common Verbs gathered...\", '\\n')\n",
    "    \n",
    "    #entityCheckCount controls how many entities are required to \n",
    "    #add a Tweet to be trained\n",
    "    \n",
    "    entityCheckCount = 3\n",
    "    \n",
    "    counter = 0\n",
    "    trainingLoop = True\n",
    "    \n",
    "    while trainingLoop:\n",
    "        counterText = \"~~~~~~~~~~~~~~~~~\"+str(counter)+\"~~~~~~~~~~~~~~~~~\"\n",
    "        print(counterText)\n",
    "        \n",
    "        nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "        try:\n",
    "            model = spacy.load(MODEL_PATH)\n",
    "            print('Model loaded...')\n",
    "        except:\n",
    "            print('No model...')\n",
    "        \n",
    "        db = DocBin() # create a DocBin object\n",
    "        \n",
    "        myTweets = []\n",
    "        \n",
    "        #Loop through all the tweets\n",
    "        #This loop is necessary to get the most common keywords \n",
    "        #in the convertToTrainingFormat function\n",
    "        \n",
    "        for i in range(len(data[0])): #len(data[0])\n",
    "            #useless if?\n",
    "#             if i % 500 == 0:\n",
    "#                 print(i)\n",
    "\n",
    "            if counter == 0:\n",
    "                x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                \n",
    "            #If counter is 1 then there is no model to check so \n",
    "            #a word count is performed\n",
    "            elif counter == 1:\n",
    "                x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                if x!= ():\n",
    "                    if len(x[1]['entities']) > entityCheckCount:\n",
    "                        #print(\"Found tweet\", x[0])\n",
    "                        myTweets.append(x)  \n",
    "            else:\n",
    "                #Convert each tweet into spacy training format\n",
    "                x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                checkPassed = False\n",
    "                if x != ():\n",
    "                    #Check the ranking of the tweet\n",
    "                    if rankTweet(x[0], model) > entityCheckCount:\n",
    "                        checkTweet = x[0].split()\n",
    "                        \n",
    "                        #Check to see if tweet has one of the common verbs\n",
    "                        for word in checkTweet:\n",
    "                            if word in commonVerbs:\n",
    "                                checkPassed = True\n",
    "                                \n",
    "                        if True: #checkPassed:\n",
    "                            #print(\"Checking rank...\")\n",
    "                            myTweets.append(x)\n",
    "        \n",
    "                        \n",
    "#         Initialize the keywords\n",
    "       \n",
    "        \n",
    "        if counter == 0:  \n",
    "            # Set keywords to be all keywords found in foodkeeper\n",
    "#            keywords = foodKeeperInfo()\n",
    "            sortedKeywords =  sorted(keywordRanker, key=keywordRanker.get, reverse=True)\n",
    "\n",
    "            for i in range(15): #sortedKeywords\n",
    "                keywords.append(sortedKeywords[i])\n",
    "            #print(sortedKeywords[i], keywordRanker[sortedKeywords[i]])\n",
    "            \n",
    "\n",
    "        elif counter > 0:\n",
    "            for text, annot in tqdm(myTweets): # data in previous format\n",
    "                doc = nlp.make_doc(text) # create doc object from text\n",
    "                ents = []\n",
    "                for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "                    span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "                    \n",
    "                    if span is None:\n",
    "                        print(\"Skipping entity\")\n",
    "                    elif ents == []: ents.append(span) \n",
    "                    else:\n",
    "                        #Check to see if any entities are overlapping i.e rice and rice cakes\n",
    "                        ents.append(span)\n",
    "                        \n",
    "                        \n",
    "#                         for ent in ents:\n",
    "#                             if ent is not None:\n",
    "#                                 entLength = ent.end - ent.start\n",
    "#                                 if span.start == ent.start or span.end == ent.end:\n",
    "#                                     if entLength > (span.end - span.start):\n",
    "#                                         continue\n",
    "#                                     else:\n",
    "#                                         ents.remove(ent)\n",
    "#                                         ents.append(span)\n",
    "#                                 else:\n",
    "#                                     ents.append(span)\n",
    "                    #if span not in ents: ents.append(span)\n",
    "                            \n",
    "                                #print(span.start, span.end, ents)\n",
    "                    #put into for loop\n",
    "#                 foundEnts = []\n",
    "#                 newEnts = []\n",
    "#                 for ent in ents:\n",
    "#                     if ent.text not in foundEnts:\n",
    "#                         foundEnts.append(ent.text)\n",
    "#                         newEnts.append(ent)\n",
    "                        \n",
    "                newEnts = filter_spans(ents)              \n",
    "                #try:\n",
    "                doc.ents = newEnts # label the text with the ents\n",
    "                    #print(doc)\n",
    "                db.add(doc)\n",
    "                #except:\n",
    "                    #print(\"Error 10: \", doc)\n",
    "\n",
    "            db.to_disk(\"./train.spacy\") # save the docbin object\n",
    "                \n",
    "            #If problems are occuring with the models not appearing\n",
    "            #ensure that the command is valid, specifically python is the correct\n",
    "            #PATH variable name on your machine\n",
    "            #--paths.train should be where the docbin object is saved\n",
    "            stream = os.popen('python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy')\n",
    "            print(stream.read())\n",
    "            print(\"Total keywords: \", str(len(keywords)))\n",
    "            print(\"List of Keywords:\\n\\n\",keywords,\"\\n\\n\")\n",
    "            oldKeywords = len(keywords)\n",
    "            \n",
    "            \n",
    "            #Look for new keywords\n",
    "            for element in myTweets:\n",
    "                keywords = findNewKeywords(element[0], keywords)\n",
    "                \n",
    "            \n",
    "            #No new keywords are found\n",
    "            if (oldKeywords == len(keywords)) and counter > 1 and entityCheckCount != 1:\n",
    "                entityCheckCount -= 1\n",
    "                print(\"Decreasing entityCheckCount variable by 1\")\n",
    "                print(\"entityCheckCount = \", entityCheckCount)\n",
    "                \n",
    "\n",
    "            #New keywords are found and entity rank check == 1\n",
    "            elif (oldKeywords == len(keywords)) and counter > 1 and entityCheckCount == 1:\n",
    "                trainingLoop = False\n",
    "            \n",
    "            eval_model()\n",
    "            \n",
    "        \n",
    "\n",
    "        #for element in myTweets:\n",
    "            #findNewKeywords(element[0], keywords)\n",
    "\n",
    "        print(\"Total keywords: \", str(len(keywords)))\n",
    "        print(\"Total Tweets: \", str(len(myTweets)))\n",
    "        print(\"List of Keywords:\\n\\n\",keywords,\"\\n\\n\")\n",
    "        counter += 1\n",
    "        \n",
    "    print('Training Done...')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def information(data):\n",
    "    myData = {}\n",
    "    totalEnt=0\n",
    "    \n",
    "    for i in range(len(data[0])):\n",
    "        doc = nlp(preProcess(data[0][i]))\n",
    "        if len(doc.ents) == 0:\n",
    "            noEntity.append(preProcess(data[0][i]))\n",
    "            \n",
    "        #print(len(doc.ents))\n",
    "        if(len(doc.ents) == 4):\n",
    "            print(doc)\n",
    "            \n",
    "        for entity in doc.ents: \n",
    "        #print(entity.label_)\n",
    "            totalEnt+=1\n",
    "            if(entity.label_ == 'FOOD'):\n",
    "                if entity.text in myData:\n",
    "                    myData[entity.text] += 1\n",
    "                else:\n",
    "                    myData[entity.text] = 1\n",
    "                    \n",
    "    print(\"Number of entities found: \" + str(len(myData)))\n",
    "    print(totalEnt)\n",
    "    for i in sorted(myData, key = myData.get):\n",
    "        print(\"Entity: \" + i, \"Count: \" + str(myData[i]), \"Density: \" + str(format(myData[i]/totalEnt, '.2f')), end = \"\\n\")\n",
    "    \n",
    "    \n",
    "    return myData\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(test_data['tweet'])):\n",
    "    test_data['tweet'][i] = preProcess(test_data['tweet'][i])\n",
    "\n",
    "y = test_data['food'].tolist()\n",
    "# print(test_data)\n",
    "# print(nlp.pipe_names)\n",
    "    \n",
    "def ent_recognize(text):\n",
    "    doc = nlp(text)\n",
    "    displacy.render(doc,style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887891bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
